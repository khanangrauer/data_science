{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#import all the required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "from keras.applications.vgg16 import decode_predictions\n",
    "from keras.preprocessing import image\n",
    "from scipy.misc import imresize\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the image and label data from the csv files (khanan's camera pictures from dentist's office)\n",
    "train_image_path=\"DentalShades/Train/Images/\"\n",
    "train_file = \"DentalShades/Train/train.csv\"\n",
    "\n",
    "train=pd.read_csv(train_file)\n",
    "\n",
    "#loading the RGB data of train images into an array\n",
    "train_img=[]\n",
    "for i in range(len(train)):\n",
    "    temp_img=image.load_img(train_image_path+train['filename'][i],target_size=(224,224))\n",
    "    temp_img=image.img_to_array(temp_img)\n",
    "    train_img.append(temp_img)\n",
    "\n",
    "#converting test images to array and applying mean subtraction processing\n",
    "train_img=np.array(train_img) \n",
    "train_img=preprocess_input(train_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# loading VGG16 model weights\n",
    "vgg_model = VGG16(weights='imagenet', include_top=False)\n",
    "\n",
    "# Extracting features from the train dataset using the VGG16 pre-trained model\n",
    "features_train=vgg_model.predict(train_img)\n",
    "\n",
    "# flattening the layers to conform to MLP input\n",
    "train_x=features_train.reshape(len(train), 25088)\n",
    "\n",
    "# converting target variable to array\n",
    "train_y=np.asarray(train['label'])\n",
    "\n",
    "# performing one-hot encoding for the target variable\n",
    "train_y=pd.get_dummies(train_y)\n",
    "train_y=np.array(train_y)\n",
    "\n",
    "#X_train, X_valid, Y_train, Y_valid=train_test_split(train_x,train_y,test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a model to train on tooth shades\n",
    "model=Sequential()\n",
    "model.add(Dense(1000, input_dim=25088, activation='relu',kernel_initializer='uniform'))\n",
    "keras.layers.core.Dropout(0.3, noise_shape=None, seed=None)\n",
    "model.add(Dense(500,input_dim=1000,activation='sigmoid'))\n",
    "keras.layers.core.Dropout(0.4, noise_shape=None, seed=None)\n",
    "model.add(Dense(150,input_dim=500,activation='sigmoid'))\n",
    "keras.layers.core.Dropout(0.2, noise_shape=None, seed=None)\n",
    "model.add(Dense(units=26))\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer=\"adam\", metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "26/26 [==============================] - 1s - loss: 3.5856 - acc: 0.0385     \n",
      "Epoch 2/20\n",
      "26/26 [==============================] - 0s - loss: 3.0891 - acc: 0.2692     \n",
      "Epoch 3/20\n",
      "26/26 [==============================] - 0s - loss: 2.9166 - acc: 0.4231     \n",
      "Epoch 4/20\n",
      "26/26 [==============================] - 0s - loss: 2.7880 - acc: 0.5769     \n",
      "Epoch 5/20\n",
      "26/26 [==============================] - 0s - loss: 2.7141 - acc: 0.8462     \n",
      "Epoch 6/20\n",
      "26/26 [==============================] - 0s - loss: 2.6410 - acc: 0.9231     \n",
      "Epoch 7/20\n",
      "26/26 [==============================] - 0s - loss: 2.5843 - acc: 0.9615     \n",
      "Epoch 8/20\n",
      "26/26 [==============================] - 1s - loss: 2.4977 - acc: 0.9615     \n",
      "Epoch 9/20\n",
      "26/26 [==============================] - 0s - loss: 2.4199 - acc: 1.0000     \n",
      "Epoch 10/20\n",
      "26/26 [==============================] - 0s - loss: 2.3365 - acc: 1.0000     \n",
      "Epoch 11/20\n",
      "26/26 [==============================] - 0s - loss: 2.2640 - acc: 0.9615     \n",
      "Epoch 12/20\n",
      "26/26 [==============================] - 0s - loss: 2.2023 - acc: 1.0000     \n",
      "Epoch 13/20\n",
      "26/26 [==============================] - 1s - loss: 2.1276 - acc: 1.0000     \n",
      "Epoch 14/20\n",
      "26/26 [==============================] - 1s - loss: 2.0627 - acc: 1.0000     \n",
      "Epoch 15/20\n",
      "26/26 [==============================] - 0s - loss: 1.9795 - acc: 1.0000     \n",
      "Epoch 16/20\n",
      "26/26 [==============================] - 0s - loss: 1.9228 - acc: 1.0000     \n",
      "Epoch 17/20\n",
      "26/26 [==============================] - 0s - loss: 1.8513 - acc: 1.0000     \n",
      "Epoch 18/20\n",
      "26/26 [==============================] - 0s - loss: 1.7869 - acc: 1.0000     \n",
      "Epoch 19/20\n",
      "26/26 [==============================] - 0s - loss: 1.7193 - acc: 1.0000     \n",
      "Epoch 20/20\n",
      "26/26 [==============================] - 0s - loss: 1.6567 - acc: 1.0000     \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1e1bc1365f8>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_x, train_y, epochs=20, batch_size=13)\n",
    "#model.fit(X_train, Y_train, epochs=100, batch_size=13,validation_data=(X_valid,Y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26/26 [==============================] - 0s\n",
      "[1.6020240783691406, 1.0]\n"
     ]
    }
   ],
   "source": [
    "#testing the exact same data as the training data (khanan's camera pictures from dentist's office)\n",
    "test_image_path=\"DentalShades/Test/Images/\"\n",
    "test_file = \"DentalShades/Test/test.csv\"\n",
    "\n",
    "test=pd.read_csv(test_file)\n",
    "\n",
    "test_img=[]\n",
    "for i in range(len(test)):\n",
    "    temp_img=image.load_img(test_image_path+train['filename'][i],target_size=(224,224))\n",
    "    temp_img=image.img_to_array(temp_img)\n",
    "    test_img.append(temp_img)\n",
    "\n",
    "#converting test images to array and applying mean subtraction processing\n",
    "test_img=np.array(test_img) \n",
    "test_img=preprocess_input(test_img)\n",
    "\n",
    "# Extracting features from the train dataset using the VGG16 pre-trained model\n",
    "features_test=vgg_model.predict(test_img)\n",
    "\n",
    "test_x=features_test.reshape(len(test), 25088)\n",
    "test_y=np.asarray(test['label'])\n",
    "test_y=pd.get_dummies(test_y)\n",
    "test_y=np.array(test_y)\n",
    "\n",
    "score = model.evaluate(test_x, test_y, batch_size=26)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#testing on the internet data now"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
